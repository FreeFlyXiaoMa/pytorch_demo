1. 张量讲解
2. Autograd：自动求导
3. 神经网络包 nn
4. 模型中的优化器 optm
5. 使用 CNN 训练一个分类模型
6. 使用 RNN 训练一个分类模型
7. 在 GPU 上跑模型以及如何使用多GPU加速训练

### 1.张量
张量是深度学习运算的基础，一切数学运算离不开张量，那么什么是张量呢？
我们都知道在机器学习中，以集成学习为例，我们做分类任务或者回归任务时，处理的都是结构化数据（离散样本或者数值型特征样本），其实在送入决策树模型
之前，程序是替我们完成了一步操作的，那就是文本向量化，只有向量化之后才能用于计算如增益率、后剪枝等操作。
那么在深度学习中，什么是Tensor呢？我们可以把Tensor看成更高级的向量---张量。
张量就是神经网络中神经元节点接收输入数据后经过一定计算操作输出的结果对象，张量在神经网络模型图中表现为各层的节点的输出数据，如果仅从结果或者数据
刘翔的角度考虑时，有时候也可以把神经网络模型中的节点看作等同于张量。
#### 1.1 创建一个随机矩阵
```
x=torch.empty(5,3) 
print(x)
```
```
输出：
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
```
这是一个为初始化的5x3矩阵，下面创建一个随机初始化矩阵(torch.rand()表示创建一个值在0~1之间的均值分布,与randn()正太分布要区别开)
```
x=torch.rand(5,3)
print(x)
```
```
输出：
tensor([[0.6972, 0.0231, 0.3087],
        [0.2083, 0.6141, 0.6896],
        [0.7228, 0.9715, 0.5304],
        [0.7727, 0.1621, 0.9777],
        [0.6526, 0.6170, 0.2605]])
```

创建一个0填充的矩阵，数据类型为long：
```
x=torch.zeros(5,3, dtype=torch.long)
print(x)
```
```
输出：
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
```
下面创建一个张量：
```
x=torch.Tensor([5.5,3])
print(x)
```
```
输出：
tensor([5.5000,3.0000])
```

根据现有的张量创建一个张量。注意：这个方法将重用输入张量的属性，如dtype，除非设置新的值进行覆盖。
```
x=x.new_ones(5,3,dtype=torch.double)
print(x)

x=torch.randn_like(x,dtype=torch.float)
print(x)
```
```
输出：
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)

tensor([[ 0.5691, -2.0126, -0.4064],
        [-0.0863,  0.4692, -1.1209],
        [-1.1177, -0.5764, -0.5363],
        [-0.4390,  0.6688,  0.0889],
        [ 1.3334, -1.1600,  1.8457]])
```
我们看一下张量x的size：
```
print(x.size())
```
```
torch.Size([5,3])
```

#### 1.2 张量运算
张量加法：
x还是上一步的x。
第一种加法是张量直接相加：
```
y=torch.rand(5,3)
print(x+y)
```
第二种是调用torch.add()函数：
```
print(torch.add(x,y))
```
第三种方法，提供输出Tensor作为参数：
```
result=torch.empty(5,3) #未初始化矩阵
torch.add(x,y,out=result)
print(result)
```
```
输出：
tensor([[ 0.7808, -1.4388,  0.3151],
        [-0.0076,  1.0716, -0.8465],
        [-0.8175,  0.3625, -0.2005],
        [ 0.2435,  0.8512,  0.7142],
        [ 1.4737, -0.8545,  2.4833]])
```
第四种，替换法：
```
#将张量x加到张量y上
y.add_(x)
print(y)
```
注意：任何 以``_`` 结尾的操作都会用结果替换原变量. 例如: ``x.copy_(y)``, ``x.t_()``, 都会改变 ``x``.
获取张量中的一部分：
```
print(x[:,1]) #获取张量的第一列所有值，和pandas的操作相同
```
```
输出：
tensor([-2.0126,  0.4692, -0.5764,  0.6688, -1.1600])
```

#### 1.3 改变Tensor的形状
调用view()函数：
```
x=torch.randn(4,4)
y=x.view(16)
z=x.view(-1,8)  #这里如果在某一维度上size为-1，那么将从其他维度上进行推断
print(x.size(),y.size(),z.size())
```
```
输出：
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
```

#### 1.4 张量与numpy数组互相转换

PyTorch中是允许将Tensor与numpy数组互换的，不过要注意，Tensor与numpy数组共享底层内存地址，修改一种数值会导致另一个的变化。
将Tensor转换为numpy数组：
```
a=torch.ones(5) #全1矩阵
b=a.numpy()

print(a)
print(b)
```
```
输出：
tensor([1., 1., 1., 1., 1.])
[1. 1. 1. 1. 1.]
```

将a中的元素加1：
```
a.add_(1)
print(a)
print(b)
```
```
输出：
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
```
数组b是由a转换而来的，所以改变了a的值，相应的b也会改变。

接着将numpy数组转换为张量：
```
import numpy as np
a=np.ones(5)
b=torch.from_numpy(a)
np.add(a,1,out=a)   #将张量a中的元素加1
print(a)
print(b)
```
```
输出：
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
```
#### 1.5 使用CUDA张量
调用``.to()`函数可以将张量移动到GPU上也可以移动到CPU上。
```
import torch
x=torch.randn(1)
if torch.cuda.is_available():   #查看是否有GPU
    device=torch.device('cuda')
    y=torch.ones_like(x,device=device)  #将张量x转为张量y--复用x的属性
    x=x.to(device)

    z=x+y
    print(z)
    print(z.to('cpu',torch.double))
```
```
输出：
tensor([3.1811], device='cuda:0')
tensor([3.1811], dtype=torch.float64)
```


### 2. Autograd：自动求导
pytorch中所有神经网络的核心是autograd包，它为张量上的所有操作提供了自动求导，并且它属于在运行时定义的框架（动态），因此梯度的反向传播是根据
代码来运行的，每次迭代可以不同。
#### 2.1 对张量求导
在创建张量时，我们通过设置requires_grad为True之后程序可以对这个张量自动求导，在程序中，pytorch会记录这个张量的每一步
操作历史并进行自动计算。
如对标量求导：
```
x=torch.rand(5,5,requires_grad=True)    #5x5的均匀分布
y=torch.rand(5,5,requires_grad=True)
z=torch.sum(x+y)
print(z)
z.backward()    #调用反向传播函数，计算梯度
print(x.grad,y.grad)
```
```
输出：
tensor(25.6487, grad_fn=<SumBackward0>)

tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]]) tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]])
```
在这里，从z的输出来看，它表示的是一个标量（只包含一个元素），那么不需要为backward()函数指定任何参数。
复杂的自动求导情况：
```
x=torch.rand(5,5,requires_grad=True)
y=torch.rand(5,5,requires_grad=True)
z=x**2+y**3
z.backward(torch.ones_like(x))  #复用张量x的属性
x=torch.rand(5,5,requires_grad=True)
y=torch.rand(5,5,requires_grad=True)
z=x**2+y**3
z.backward(torch.ones_like(x))  #复用张量x的属性
print(x)
print(x.grad)
print(x.grad/x)
```
```
输出：
tensor([[0.3628, 0.2263, 0.3792, 0.7198, 0.2116],
        [0.8284, 0.4264, 0.0305, 0.8310, 0.6624],
        [0.2340, 0.4949, 0.3748, 0.5335, 0.9700],
        [0.5050, 0.8810, 0.1832, 0.9523, 0.6255],
        [0.0169, 0.8306, 0.4583, 0.9238, 0.3482]], requires_grad=True)
tensor([[0.7256, 0.4526, 0.7585, 1.4396, 0.4232],
        [1.6569, 0.8528, 0.0609, 1.6620, 1.3248],
        [0.4680, 0.9898, 0.7497, 1.0669, 1.9400],
        [1.0099, 1.7620, 0.3665, 1.9046, 1.2509],
        [0.0338, 1.6612, 0.9167, 1.8476, 0.6965]])
#这里通过z对x的导数正好是x的两倍
tensor([[2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2.]], grad_fn=<DivBackward0>)
```
我们来看一下对x的求导过程：
- 首先变量x与y属于无关变量，那么只需要z对x求偏微分即可
$$x.grad={\partial z }\over {\partial x}={\partial x**2}\over {\partial x}=2*x$$

那么计算梯度有什么用呢？那就是为了更新x的值$x_new=x_old - \Delta x$,求梯度的作用将在第三小节反向传播中讲到。

#### 2.2 禁用求导
有时候在计算准确率的时候会考虑不让网络层更新权重，那么就需要禁止模型求导，通过调用with torch.no_grad()上下文管理器可以临时禁止对requires_grad的张量进行自动求导：
```
with torch.no_grad():
    print((x+y*2).requires.grad)
```
```
False
```


#### 2.3 前向传播与反向传播
这里我们通过计算$y=x*5$体验一下前向传播与反向传播的过程
1. 前向传播：就是计算x与5相乘的结果，调用forward()函数
2. 反向传播：求${{\partial y}\over{\partial x}}$的导数，调用backward()函数
```
from torch.autograd.function import Function

class Separate(Function):
    @staticmethod
    def forward(ctx,tensor,constant):
        ctx.constant=constant
        return tensor*constant
    @staticmethon
    def backward(ctx,grad_output):
        return grad_output,None     #返回值分别对应输入的tensor和constant的梯度
```
测试一下：
```
x=torch.rand(2,2,requires_grad=True)
y=Separate.apply(x,5)
print('a：',x)
print('b：',y)
```
```
输出结果：
a： tensor([[0.5555, 0.9865],
        [0.6241, 0.4869]], requires_grad=True)
b： tensor([[2.7774, 4.9325],
        [3.1207, 2.4346]], grad_fn=<SeparateBackward>)

```
其实上一步$Separate.apply(x,5)$已经完成了前向传播，下面计算反向传播：
```
y.backward(torch.ones_like(x))
print(x.grad)

输出：
tensor([[1., 1.],
        [1., 1.]])
```
注意：反向传播时如果不给$y.backward()$传一个标量，那么会报错：
$RuntimeError: grad can be implicitly created only for scalar outputs$

#### 2.4 小结
我们来总结一下，首先我们根据网络的输出对每一个变量求导，根据偏导数我们反向传播来更新每一个变量，这样一轮下来（epoch的概念）
权重矩阵中的每一个变量得到了更新（偏置不变）。
然后我们不断求导不断进行反向传播来更新变量值，总会在某一个点达到收敛，那么这时网络层参数达到最优，这就是我们想要的效果，模型训练就完成了。



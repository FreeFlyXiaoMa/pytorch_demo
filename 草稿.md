1. 张量讲解
2. Autograd：自动求导
3. 神经网络包 nn
4. 模型中的优化器 optm
5. 使用 CNN 训练一个分类模型
6. 使用 RNN 训练一个分类模型
7. 在 GPU 上跑模型以及如何使用多GPU加速训练

### 1.张量
张量是深度学习运算的基础，一切数学运算离不开张量，那么什么是张量呢？
我们都知道在机器学习中，以集成学习为例，我们做分类任务或者回归任务时，处理的都是结构化数据（离散样本或者数值型特征样本），其实在送入决策树模型
之前，程序是替我们完成了一步操作的，那就是文本向量化，只有向量化之后才能用于计算如增益率、后剪枝等操作。
那么在深度学习中，什么是Tensor呢？我们可以把Tensor看成更高级的向量---张量。
张量就是神经网络中神经元节点接收输入数据后经过一定计算操作输出的结果对象，张量在神经网络模型图中表现为各层的节点的输出数据，如果仅从结果或者数据
刘翔的角度考虑时，有时候也可以把神经网络模型中的节点看作等同于张量。
#### 1.1 创建一个随机矩阵
```
x=torch.empty(5,3) 
print(x)
```
```
输出：
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
```
这是一个为初始化的5x3矩阵，下面创建一个随机初始化矩阵(torch.rand()表示创建一个值在0~1之间的均值分布,与randn()正太分布要区别开)
```
x=torch.rand(5,3)
print(x)
```
```
输出：
tensor([[0.6972, 0.0231, 0.3087],
        [0.2083, 0.6141, 0.6896],
        [0.7228, 0.9715, 0.5304],
        [0.7727, 0.1621, 0.9777],
        [0.6526, 0.6170, 0.2605]])
```

创建一个0填充的矩阵，数据类型为long：
```
x=torch.zeros(5,3, dtype=torch.long)
print(x)
```
```
输出：
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
```
下面创建一个张量：
```
x=torch.Tensor([5.5,3])
print(x)
```
```
输出：
tensor([5.5000,3.0000])
```

根据现有的张量创建一个张量。注意：这个方法将重用输入张量的属性，如dtype，除非设置新的值进行覆盖。
```
x=x.new_ones(5,3,dtype=torch.double)
print(x)

x=torch.randn_like(x,dtype=torch.float)
print(x)
```
```
输出：
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)

tensor([[ 0.5691, -2.0126, -0.4064],
        [-0.0863,  0.4692, -1.1209],
        [-1.1177, -0.5764, -0.5363],
        [-0.4390,  0.6688,  0.0889],
        [ 1.3334, -1.1600,  1.8457]])
```
我们看一下张量x的size：
```
print(x.size())
```
```
torch.Size([5,3])
```

#### 1.2 张量运算
张量加法：
x还是上一步的x。
第一种加法是张量直接相加：
```
y=torch.rand(5,3)
print(x+y)
```
第二种是调用torch.add()函数：
```
print(torch.add(x,y))
```
第三种方法，提供输出Tensor作为参数：
```
result=torch.empty(5,3) #未初始化矩阵
torch.add(x,y,out=result)
print(result)
```
```
输出：
tensor([[ 0.7808, -1.4388,  0.3151],
        [-0.0076,  1.0716, -0.8465],
        [-0.8175,  0.3625, -0.2005],
        [ 0.2435,  0.8512,  0.7142],
        [ 1.4737, -0.8545,  2.4833]])
```
第四种，替换法：
```
#将张量x加到张量y上
y.add_(x)
print(y)
```
注意：任何 以``_`` 结尾的操作都会用结果替换原变量. 例如: ``x.copy_(y)``, ``x.t_()``, 都会改变 ``x``.
获取张量中的一部分：
```
print(x[:,1]) #获取张量的第一列所有值，和pandas的操作相同
```
```
输出：
tensor([-2.0126,  0.4692, -0.5764,  0.6688, -1.1600])
```

#### 1.3 改变Tensor的形状
调用view()函数：
```
x=torch.randn(4,4)
y=x.view(16)
z=x.view(-1,8)  #这里如果在某一维度上size为-1，那么将从其他维度上进行推断
print(x.size(),y.size(),z.size())
```
```
输出：
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
```

#### 1.4 张量与numpy数组互相转换

PyTorch中是允许将Tensor与numpy数组互换的，不过要注意，Tensor与numpy数组共享底层内存地址，修改一种数值会导致另一个的变化。
将Tensor转换为numpy数组：
```
a=torch.ones(5) #全1矩阵
b=a.numpy()

print(a)
print(b)
```
```
输出：
tensor([1., 1., 1., 1., 1.])
[1. 1. 1. 1. 1.]
```

将a中的元素加1：
```
a.add_(1)
print(a)
print(b)
```
```
输出：
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
```
数组b是由a转换而来的，所以改变了a的值，相应的b也会改变。

接着将numpy数组转换为张量：
```
import numpy as np
a=np.ones(5)
b=torch.from_numpy(a)
np.add(a,1,out=a)   #将张量a中的元素加1
print(a)
print(b)
```
```
输出：
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
```
#### 1.5 使用CUDA张量
调用``.to()`函数可以将张量移动到GPU上也可以移动到CPU上。
```
import torch
x=torch.randn(1)
if torch.cuda.is_available():   #查看是否有GPU
    device=torch.device('cuda')
    y=torch.ones_like(x,device=device)  #将张量x转为张量y--复用x的属性
    x=x.to(device)

    z=x+y
    print(z)
    print(z.to('cpu',torch.double))
```
```
输出：
tensor([3.1811], device='cuda:0')
tensor([3.1811], dtype=torch.float64)
```
